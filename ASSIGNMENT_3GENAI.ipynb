{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiFiMTeiDalXjejMG0bF18",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52440/GenerativeAi_2025/blob/main/ASSIGNMENT_3GENAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlw7pWzQut_J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1\n",
        "(1 ponto) Write Python code without using any libraries to find the value of x at which the function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm."
      ],
      "metadata": {
        "id": "pNRuyLRju8kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "\n",
        "def gradient_descent_1d(function, initial_x, learning_rate, num_iterations):\n",
        "    x = sp.symbols('x')\n",
        "    function = sp.sympify(function)\n",
        "    gradient_fn = sp.diff(function, x)\n",
        "    xi = initial_x\n",
        "    # Perform gradient descent\n",
        "    for i in range(num_iterations):\n",
        "        gradient = gradient_fn.subs(x, xi)\n",
        "        xi = xi - learning_rate * gradient\n",
        "        # Early stopping condition\n",
        "        if abs(gradient) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "    return xi.evalf(), function.subs(x, xi).evalf()\n",
        "\n",
        "\n",
        "# Input for Question 1\n",
        "# function = \"5*x**4 + 3*x**2 + 10\"\n",
        "# initial_x = float(input(\"Enter the initial value of x for Question 1: \"))\n",
        "# learning_rate = float(input(\"Enter the learning rate for Question 1: \"))\n",
        "# num_iterations = int(input(\"Enter the number of iterations for Question 1: \"))\n",
        "\n",
        "function = \"5*x**4 + 3*x**2 + 10\"\n",
        "initial_x = 2\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Execute Gradient Descent for Question 1\n",
        "x_min, min_value = gradient_descent_1d(function, initial_x, learning_rate, num_iterations)\n",
        "\n",
        "# Output Results for Question 1\n",
        "print(f\"Question 1: Minimum value of f(x) is {min_value} at x = {x_min}.\")\n"
      ],
      "metadata": {
        "id": "yKuBGFYJu9cp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab3fe29-744d-429e-b01f-0ea883869612"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at iteration 232\n",
            "Question 1: Minimum value of f(x) is 10.0000000000001 at x = 1.52885214797255E-7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write Python code without using any libraries to find the value of x and y at which the function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm.\n",
        "\n",
        "  f(x) = 3x^2 +5e^−y +10"
      ],
      "metadata": {
        "id": "lOAlP2lHvYPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_2d(function, initial_x, initial_y, learning_rate, num_iterations):\n",
        "\n",
        "    x, y = sp.symbols('x y')\n",
        "    function = sp.sympify(function)\n",
        "    gradient_x = sp.diff(function, x)\n",
        "    gradient_y = sp.diff(function, y)\n",
        "    xi, yi = initial_x, initial_y\n",
        "    for i in range(num_iterations):\n",
        "        grad_x = gradient_x.subs({x: xi, y: yi})\n",
        "        grad_y = gradient_y.subs({x: xi, y: yi})\n",
        "        xi = xi - learning_rate * grad_x\n",
        "        yi = yi - learning_rate * grad_y\n",
        "        if abs(grad_x) < 1e-6 and abs(grad_y) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "    return xi.evalf(), yi.evalf(), function.subs({x: xi, y: yi}).evalf()\n",
        "\n",
        "\n",
        "# Input for Question 2\n",
        "# function = \"3*x**2 + 5*sp.exp(-y) + 10\"\n",
        "# initial_x = float(input(\"Enter the initial value of x for Question 2: \"))\n",
        "# initial_y = float(input(\"Enter the initial value of y for Question 2: \"))\n",
        "# learning_rate = float(input(\"Enter the learning rate for Question 2: \"))\n",
        "# num_iterations = int(input(\"Enter the number of iterations for Question 2: \"))\n",
        "\n",
        "function = \"3*x**2 + 5*exp(-y) + 10\"\n",
        "initial_x = 1\n",
        "initial_y = 1\n",
        "learning_rate = 0.1\n",
        "num_iterations = 500\n",
        "\n",
        "# Execute Gradient Descent for Question 2\n",
        "x_min, y_min, min_value = gradient_descent_2d(function, initial_x, initial_y, learning_rate, num_iterations)\n",
        "\n",
        "# Output Results for Question 2\n",
        "print(f\"Question 2: Minimum value of g(x, y) is {min_value} at x = {x_min}, y = {y_min}.\")"
      ],
      "metadata": {
        "id": "Ps-3Zj3ZvZKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write Python code without using any libraries to find the value of x at which the sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent Algorithm.\n",
        "\n",
        "  z(x) = 1/(1+e^-x)"
      ],
      "metadata": {
        "id": "P1AoXbMswOwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_sigmoid(initial_x, learning_rate, num_iterations):\n",
        "    x = sp.symbols('x')\n",
        "    function = 1 / (1 + sp.exp(-x))\n",
        "    gradient_fn = sp.diff(function, x)\n",
        "    xi = initial_x\n",
        "    for i in range(num_iterations):\n",
        "        gradient = gradient_fn.subs(x, xi)\n",
        "        xi = xi - learning_rate * gradient\n",
        "        if abs(gradient) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "    return xi.evalf(), function.subs(x, xi).evalf()\n",
        "\n",
        "\n",
        "# Input for Question 3\n",
        "# initial_x = float(input(\"Enter the initial value of x for Question 3: \"))\n",
        "# learning_rate = float(input(\"Enter the learning rate for Question 3: \"))\n",
        "# num_iterations = int(input(\"Enter the number of iterations for Question 3: \"))\n",
        "\n",
        "initial_x = 5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 100\n",
        "# Execute Gradient Descent for Question 3\n",
        "x_min, min_value = gradient_descent_sigmoid(initial_x, learning_rate, num_iterations)\n",
        "\n",
        "# Output Results for Question 3\n",
        "print(f\"Question 3: Minimum value of z(x) is {min_value} at x = {x_min}.\")\n",
        ""
      ],
      "metadata": {
        "id": "z0zSJJuFwP35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write Python code without using any libraries to find the value of optimal values of model parameters M and C such that the model’s Square Error Value shown in equation 4 will be minimum. It means model gives output close to expected output as shown in Figure 1"
      ],
      "metadata": {
        "id": "ZFn5Gm6pwaFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_descent_linear_model(inputs, expected_outputs, learning_rate, num_iterations):\n",
        "    M, C = 0, 0\n",
        "    n = len(inputs)\n",
        "    for i in range(num_iterations):\n",
        "        gradient_M = sum(-2 * x * (y - (M * x + C)) for x, y in zip(inputs, expected_outputs)) / n\n",
        "        gradient_C = sum(-2 * (y - (M * x + C)) for x, y in zip(inputs, expected_outputs)) / n\n",
        "        M = M - learning_rate * gradient_M\n",
        "        C = C - learning_rate * gradient_C\n",
        "        if abs(gradient_M) < 1e-6 and abs(gradient_C) < 1e-6:\n",
        "            print(f\"Converged at iteration {i+1}\")\n",
        "            break\n",
        "\n",
        "    return M, C\n",
        "\n",
        "\n",
        "# Inputs and expected outputs for Question 4\n",
        "inputs = [1, 2, 3, 4]\n",
        "expected_outputs = [2, 4, 6, 8]\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Execute Gradient Descent for Question 4\n",
        "M_opt, C_opt = gradient_descent_linear_model(inputs, expected_outputs, learning_rate, num_iterations)\n",
        "\n",
        "# Output results for Question 4\n",
        "print(f\"Question 4: Optimal values are M = {M_opt}, C = {C_opt}.\")\n",
        ""
      ],
      "metadata": {
        "id": "GmcX4Av_wf6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}